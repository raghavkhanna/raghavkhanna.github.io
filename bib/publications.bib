@article{snyder2012improved,
  title={Improved thermoelectric cooling based on the Thomson effect},
  author={Snyder, G Jeffrey and Toberer, Eric S and Khanna, Raghav and Seifert, Wolfgang},
  journal={Physical Review B},
  volume={86},
  number={4},
  pages={045202},
  year={2012},
  publisher={APS},
  abstract={Traditional thermoelectric Peltier coolers exhibit a cooling limit which is primarily determined by the figure of merit, zT. Rather than a fundamental thermodynamic limit, this bound can be traced to the difficulty of maintaining thermoelectric compatibility. Self-compatibility locally maximizes the cooler's coefficient of performance for a given zT and can be achieved by adjusting the relative ratio of the thermoelectric transport properties that make up zT. In this study, we investigate the theoretical performance of thermoelectric coolers that maintain self-compatibility across the device. We find that such a device behaves very differently from a Peltier cooler, and we term self-compatible coolers “Thomson coolers” when the Fourier heat divergence is dominated by the Thomson, as opposed to the Joule, term. A Thomson cooler requires an exponentially rising Seebeck coefficient with increasing temperature, while traditional Peltier coolers, such as those used commercially, have comparatively minimal change in Seebeck coefficient with temperature. When reasonable material property bounds are placed on the thermoelectric leg, the Thomson cooler is predicted to achieve approximately twice the maximum temperature drop of a traditional Peltier cooler with equivalent figure of merit (zT). We anticipate that the development of Thomson coolers will ultimately lead to solid-state cooling to cryogenic temperatures.},
  url={https://journals.aps.org/prb/pdf/10.1103/PhysRevB.86.045202},
  categories={journal}
}

@mastersthesis{khanna2013storing,
  title={Storing Sunlight: Experimental Investigation of a Combined Sensible and Latent Heat Storage for Concentrated Solar Power Plants},
  author={Khanna, Raghav},
  year={2013},
  school={ETH-Z{\"u}rich},
  abstract={This report presents the heat transfer modelling, design, construction and experimental in- vestigation of a 40.29 KWhrth laboratory scale combined latent and sensible heat storage for concentrated solar power. The combined storage consists of a 3.97 KWhrth “latent” heat stor- age section, containing the eutectic alloy of Aluminium and Silicon, AlSi12, encapsulated in stainless steel tubes, placed on top of a 36.32 KWhrth “sensible” heat storage section, compris- ing of a packed bed of rocks. Adding a thin section of phase change material, comprising less than 7% of the total storage volume, on top of a sensible heat storage is observed to provide a highly stabilised outlet air temperature during the storage discharge cycle. Experiments show that, for a comparable range of air mass flow rates, the discharge outlet air temperature for the combined storage decreases by only 10-15 ◦C, in comparison to a 72-112 ◦C drop in tem- perature observed for a sensible only storage with the same volume, over the same time period. The combined storage concept can hence be used to provide heat at approximately constant temperatures over a significant time period for power generation or process heat applications, at little added cost, while maintaining the high thermodynamic efficiencies characteristic of thermally stratified single tank sensible heat storages.},
  url={https://www.research-collection.ethz.ch/handle/20.500.11850/154315},
  categories={master-thesis}
}

@article{zanganeh2015experimental,
  title={Experimental and numerical investigation of combined sensible-latent heat for thermal energy storage at 575 C and above},
  author={Zanganeh, G and Khanna, R and Walser, C and Pedretti, A and Haselbacher, A and Steinfeld, A},
  journal={Solar Energy},
  volume={114},
  pages={77--90},
  year={2015},
  publisher={Pergamon},
  abstract={The design, testing, and modelling of a high-temperature thermocline-type thermal energy storage (TES) are presented. The TES concept uses air as the heat-transfer fluid and combines sensible and latent heat for stabilizing the discharging outflow air temperature. A 42 kWhth lab-scale prototype of 40 cm diameter was fabricated, containing a 9 cm high layer of encapsulated phase change material (AlSi12) on top of a 127 cm high packed bed of sedimentary rocks with a mean diameter of about 3 cm. A two-phase transient heat transfer model of the thermal storage cycle was numerically formulated and experimentally validated with measured thermoclines during charging and discharging obtained with the lab-scale prototype. The thermal inertia of the experimental setup and the radial variation of void fraction due to the small tank-to-particle diameter ratio affected the validation process. The outflow air temperature during discharging was stabilized around the melting temperature of AlSi12 of 575 °C. The thermal losses stayed below 3.5% of the input energy for all the experimental runs.},
  url={http://www.sciencedirect.com/science/article/pii/S0038092X15000365},
  categories={journal}
}

@inproceedings{khanna2015beyond,
  title={Beyond point clouds-3d mapping and field parameter measurements using uavs},
  author={Khanna, Raghav and Möller, Martin and Pfeifer, Johannes and Liebisch, Frank and Walter, Achim and Siegwart, Roland},
  booktitle={Emerging Technologies \& Factory Automation (ETFA), 2015 IEEE 20th Conference on},
  pages={1--4},
  year={2015},
  url={http://ieeexplore.ieee.org/document/7301583/},
  abstract={Recent developments in Unmanned Aerial Vehicles (UAVs) have made them ideal tools for remotely monitoring agricultural fields. Complementary advancements in computer vision have enabled automated post-processing of images to generate dense 3D reconstructions in the form of point clouds. In this paper we present a monitoring pipeline that uses a readily available, low cost UAV and camera for quickly surveying a winter wheat field, generate a 3D point cloud from the collected imagery and present methods for automated crop height estimation from the extracted point cloud and compare our estimates with those using standardized techniques.},
  organization={IEEE},
  categories={conference}
}

@article{khannastudying,
  title={Studying Phenotypic Variability in Crops using a Hand-held Sensor Platform},
  author={Khanna, Raghav and Rehder, Joern and Möller, Martin and Galceran, Enric and Siegwart, Roland},
  year={2015},
  abstract={Recent developments in visual-inertial and LiDAR sensors and simultaneous localization and mapping (SLAM) enable recording and digital reconstruction of the physical world. In this paper we utilize a hand-held multi-sensor platform for remotely recording and characterizing physical properties of crops on a field. The platform consists of a visual-inertial sensor, color camera and 2D LiDAR. We syncronize the data from this platform and fuse them in a standard SLAM framework to obtain a detailed model of the field environment in the form of a 3D point cloud. Such a model is then fed into semi-automated crop parameter estimation pipelines to extract the spatio-temporal variation of physical crop height and canopy cover, which may be used to support decision making for breeding and precision agriculture. We present experimental results with data collected on a winter wheat field in Eschikon, Switzerland, showing the utility of our approach towards automating variability studies in crops.},
  url={http://flourish-project.eu/fileadmin/user_upload/publications/khanna2015iros_afr.pdf},
  categories={workshop}
}

@article{liebischflourish,
  title={Flourish-A robotic approach for automation in crop management},
  author={Liebisch, Frank and Pfeifer, Johannes and Khanna, Raghav and Lottes, Philipp and Stachniss, Cyrill and Falck, Tillmann and Sander, Slawomir and Siegwart, Roland and Walter, Achim and Galceran, Enric},
  year={2016},
  abstract={The goal of the Flourish project is to bridge the gap between the current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. Thereby, combining the aerial survey capabilities of a small autono- mous multi-copter Unmanned Aerial Vehicle (UAV) with a multi-purpose agricultural Unmanned Ground Vehicle (UGV), the system will be able to survey a field from the air, perform targeted intervention on the ground, and provide detailed information for deci- sion support, all with minimal user intervention. The system can be adapted to a wide range of farm management activities and different crops by choosing different sensors, status indicators and ground treatment packages. The gathered information can be used alongside existing precision agriculture machinery, for example, by providing posi- tion maps for variable rate fertilizer application.},
  url={http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/liebisch16cbaws.pdf},
  categories={conference}
  }

@inproceedings{pfeifer2016towards,
  title={Towards automatic UAV data interpretation for precision farming},
  author={Pfeifer, Johannes and Khanna, Raghav and Dragos, C and Popovic, Marija and Galceran, Enric and Kirchgessner, Norbert and Walter, Achim and Siegwart, Roland and Liebisch, Frank},
  booktitle={CIGR-AgEng conference. Aarhus, Denmark},
  abstract={Background: The EU-project Flourish intends to establish an autonomously operating precision farming system based on the interaction between unmanned ground vehicles (UGVs) and unmanned aerial vehicles (UAVs). For effective mission planning and site-specific ground intervention by the UGV, the growth, mineral nutrition, weed, and health status of the crop field must be evaluated efficiently. In this regard, the survey capabilities of UAVs can substantially leverage the economic performance and ecological sustainability of precision farming systems.
    Methods: Our approach is based on ‘sufficient performance ranges’ (SPRs), which represent the expected optimal performance of phenotypic traits such as spectral indicators and growth parameters like canopy cover and crop height. Together with a priori data, such as weather situation and soil fertility maps, the UAV derived maps allow for the detection of deviations from sufficient crop development. Detected deviations are interpreted using decision tree models. Our models encompass upstream and downstream decisions necessary for scheduling site-specific and efficient ground interventions during the whole management sequence of a growing season, such as fertilizer input or weed control.
    Results: This contribution presents initial results for field monitoring via UAVs. The performance of our approach is supported by ground truth data, such as crop height, canopy cover and spectral indices from sugar beets collected in 2015. Effects of variable soil fertility, weed pressure and drought stress are presented.
    Conclusion: The initial results support the proposed intention to derive appropriate management decisions for stabilizing crop yield and quality while minimizing farm inputs.},
    url={http://flourish-project.eu/fileadmin/user_upload/publications/2016_Pfeifer-UAV_data_interpretation.pdf},
  year={2016},
  categories={conference}
}

@inproceedings{lottes2017uav,
  title={UAV-based crop and weed classification for smart farming},
  author={Lottes, Philipp and Khanna, Raghav and Pfeifer, Johannes and Siegwart, Roland and Stachniss, Cyrill},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={3024--3031},
  year={2017},
  abstract={Unmanned aerial vehicles (UAVs) and other robots in smart farming applications offer the potential to monitor farm land on a per-plant basis, which in turn can reduce the amount of herbicides and pesticides that must be applied. A central information for the farmer as well as for autonomous agriculture robots is the knowledge about the type and distribu- tion of the weeds in the field. In this regard, UAVs offer excellent survey capabilities at low cost. In this paper, we address the problem of detecting value crops such as sugar beets as well as typical weeds using a camera installed on a light-weight UAV. We propose a system that performs vegetation detection, plant-tailored feature extraction, and classification to obtain an estimate of the distribution of crops and weeds in the field. We implemented and evaluated our system using UAVs on two farms, one in Germany and one in Switzerland and demonstrate that our approach allows for analyzing the field and classifying individual plants.},
  url={http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes17icra.pdf},
  organization={IEEE},
  categories={conference}
}

@inproceedings{khanna2017field,
  title={On field radiometric calibration for multispectral cameras},
  author={Khanna, Raghav and Sa, Inkyu and Nieto, Juan and Siegwart, Roland},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={6503--6509},
  year={2017},
  url={http://flourish-project.eu/fileadmin/user_upload/publications/2017-icra-khanna.pdf},
  abstract={Perception systems for outdoor robotics have to deal with varying environmental conditions. Variations in illumination in particular, are currently the biggest challenge for vision-based perception. In this paper we present an approach for radiometric characterization of multispectral cameras. To enable spatio-temporal mapping we also present a procedure for in-situ illumination estimation, resulting in radiometric calibration of the collected images. In contrast to current approaches, we present a purely data driven, parameter free approach, based on maximum likelihood estimation which can be performed entirely on the field, without requiring specialised laboratory equipment. Our routine requires three simple datasets which are easily acquired using most modern multispectral cameras. We evaluate the framework with a cost-effective snapshot multispectral camera. The results show that our method enables the creation of quatitatively accurate relative reflectance images with challenging on field calibration datasets under a variety of ambient conditions.},
  organization={IEEE},
  categories={conference}
}

@article{sa2017build,
  title={Build your own visual-inertial odometry aided cost-effective and open-source autonomous drone},
  author={Sa, Inkyu and Kamel, Mina and Burri, Michael and Bloesch, Michael and Khanna, Raghav and Popovic, Marija and Nieto, Juan and Siegwart, Roland},
  journal={arXiv preprint arXiv:1708.06652},
  abstract={This paper describes an approach to building a cost-effective and research grade visual-inertial odometry aided vertical taking-off and landing (VTOL) platform. We utilize an off-the-shelf visual-inertial sensor, an onboard computer, and a quadrotor platform that are factory-calibrated and mass-produced, thereby sharing similar hardware and sensor specifications (e.g., mass, dimensions, intrinsic and extrinsic of camera-IMU systems, and signal-to-noise ratio). We then perform a system calibration and identification enabling the use of our visual-inertial odometry, multi-sensor fusion, and model predictive control frameworks with the off-the-shelf products. This implies that we can partially avoid tedious parameter tuning procedures for building a full system. The complete system is extensively evaluated both indoors using a motion capture system and outdoors using a laser tracker while performing hover and step responses, and trajectory following tasks in the presence of external wind disturbances. We achieve root-mean-square (RMS) pose errors between a reference and actual trajectories of 0.036m, while performing hover. We also conduct relatively long distance flight (~180m) experiments on a farm site and achieve 0.82% drift error of the total distance flight. This paper conveys the insights we acquired about the platform and sensor module and returns to the community as open-source code with tutorial documentation.},
  url={https://arxiv.org/abs/1708.06652},
  year={2017},
  code={https://github.com/ethz-asl/mav_dji_ros_interface},
  categories={conference}
}

@article{sa2017dynamic,
  title={Dynamic System Identification, and Control for a cost effective open-source VTOL MAV},
  author={Sa, Inkyu and Kamel, Mina and Khanna, Raghav and Popovic, Marija and Nieto, Juan and Siegwart, Roland},
  journal={arXiv preprint arXiv:1701.08623},
  year={2017},
  url={https://arxiv.org/abs/1701.08623},
  abstract={This paper describes dynamic system identification, and full control of a cost-effective vertical take-off and landing (VTOL) multi-rotor micro-aerial vehicle (MAV) --- DJI Matrice 100. The dynamics of the vehicle and autopilot controllers are identified using only a built-in IMU and utilized to design a subsequent model predictive controller (MPC). Experimental results for the control performance are evaluated using a motion capture system while performing hover, step responses, and trajectory following tasks in the present of external wind disturbances. We achieve root-mean-square (RMS) errors between the reference and actual trajectory of x=0.021m, y=0.016m, z=0.029m, roll=0.392deg, pitch=0.618deg, and yaw=1.087deg while performing hover. This paper also conveys the insights we have gained about the platform and returned to the community through open-source code, and documentation.},
  code={https://github.com/ethz-asl/mav_dji_ros_interface}
}

@article{sa2017weednet,
  title={weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming},
  author={Sa, Inkyu and Chen, Zetao and Popovic, Marija and Khanna, Raghav and Liebisch, Frank and Nieto, Juan and Siegwart, Roland},
  journal={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  abstract={Selective weed treatment is a critical step in autonomous crop management as related to crop health and yield. However, a key challenge is reliable, and accurate weed detection to minimize damage to surrounding plants. In this paper, we present an approach for dense semantic weed classification with multispectral images collected by a micro aerial vehicle (MAV). We use the recently developed encoder-decoder cascaded Convolutional Neural Network (CNN), Segnet, that infers dense semantic classes while allowing any number of input image channels and class balancing with our sugar beet and weed datasets. To obtain training datasets, we established an experimental field with varying herbicide levels resulting in field plots containing only either crop or weed, enabling us to use the Normalized Difference Vegetation Index (NDVI) as a distinguishable feature for automatic ground truth generation. We train 6 models with different numbers of input channels and condition (fine-tune) it to achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested for MAV integration. Dataset used in this paper is released to support the community and future work.},
  url={https://arxiv.org/abs/1709.03329},
  year={2017},
  code={https://github.com/inkyusa/weedNet},
  categories={conference}
}

@inproceedings{sommer2017calibration_devices,
  title={A Low-Cost System for High-Rate, High-Accuracy Temporal Calibration for LIDARs and Cameras},
  author={Sommer, Hannes and Khanna, Raghav and Gilitschenski, Igor and Taylor, Zachary and Siegwart, Roland and Nieto, Juan},
  booktitle={Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
  pages={},
  year={2017},
  abstract={Deployment of camera and laser based motion estimation systems for controlling platforms operating at high speeds, such as cars or trains, is posing increasingly challenging precision requirements on the temporal calibration of these sensors. In this work, we demonstrate a simple, low-cost system for calibrating any combination of cameras and time of flight LIDARs with respect to the CPU clock (and therefore, also to each other). The newly proposed device is based on widely available off-the-shelf components, such as the Raspberry Pi 3, which is synchronized using the Precision Time Protocol (PTP) with respect to the CPU of the sensor carrying system. The obtained accuracy can be shown to be below 0.1 ms per measurement for LIDARs and below minimal exposure time per image for cameras. It outperforms state-of-the-art approaches also not relying on hardware synchronization by more than a factor of 10 in precision. Moreover, the entire process can be carried out at a high rate allowing the study of how offsets evolve over time. In our analysis, we demonstrate how each building block of the system contributes to this accuracy and validate the obtained results using real-world data.},
  url={http://flourish-project.eu/fileadmin/user_upload/publications/2017-iros-sommer.pdf},
  code={https://github.com/ethz-asl/cuckoo_time_translator},
  organization={IEEE},
  categories={conference}
}

@article{funk2017autonomous_electric,
  title={Autonomous Electric Race Car Design},
  author={Niklas Funk and Nikhilesh Alatur and Robin Deuber and Frederick Gonon and Nico Messikommer and Julian Nubert and Moritz Patriarca and Simon Schaefer and Dominic Scotoni and Nicholas Bünger and Renaud Dube and Raghav Khanna and Mark Pfeiffer and Erik Wilhelm and Roland Siegwart},
  journal={EVS30 Symposium 2017},
  abstract={Autonomous driving and electric vehicles are nowadays very active research and development areas. In this paper we present the conversion of a standard Kyburz eRod into an autonomous vehicle that can be operated in challenging environments such as Swiss mountain passes. The overall hardware and software architectures are described in detail with a special emphasis on the sensor requirements for autonomous vehicles operating in partially structured environments. Furthermore, the design process itself and the finalized system architecture are presented. The work shows state of the art results in localization and controls for self-driving high-performance electric vehicles. Test results of the overall system are presented, which show the importance of generalizable state estimation algorithms to handle a plethora of conditions.},
  url={https://arxiv.org/abs/1711.00548},
  year={2017},
  code={http://www.project-arc.ch},
  categories={accepted}
}

@article{sa2018weedmap,
  title={WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming},
  author={Sa, Inkyu and Popovic, Marija and Khanna, Raghav and Chen, Zetao and Lottes, Philipp and Liebisch, Frank and Nieto, Juan and Stachniss, Cyrill and Walter, Achim and Siegwart, Roland},
  journal={Remote Sensing},
  url={http://www.mdpi.com/2072-4292/10/9/1423},
  abstract={The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB (red, green, and blue) inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.},
  volume={10},
  number={9},
  pages={1423},
  year={2018},
  code={https://projects.asl.ethz.ch/datasets/doku.php?id=weedmap:remotesensing2018weedmap},
  publisher={Multidisciplinary Digital Publishing Institute},
  categories={journal}
}

@inproceedings{walter2018flourish,
  title={Flourish-a robotic approach for automation in crop management},
  author={Walter, Achim and Khanna, Raghav and Lottes, Philipp and Stachniss, Cyrill and Siegwart, Roland and Nieto, Juan and Liebisch, Frank},
  booktitle={Proceedings of the International Conference on Precision Agriculture (ICPA)},
  year={2018},
  categories={conference},
  url={https://www.ispag.org/proceedings/?action=abstract&id=5051},
  abstract={The Flourish project aims to bridge the gap between current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. Combining the aerial survey capabilities of a small autonomous multi-copter Unmanned Aerial Vehicle (UAV) with a multi-purpose agricultural Unmanned Ground Vehicle (UGV), the system will be able to survey a field from the air, perform targeted intervention on the ground, and provide detailed information for decision support, all with minimal user intervention. The system can be adapted to a wide range of farm management activities and to different crops by choosing different sensors, status indicators and ground treatment packages. The research project thereby touches a selection of topics addressed by ICPA such as sensor application in managing in-season crop variability, precision nutrient management and crop protection as well as remote sensing applications in precision agriculture and engineering technologies and advances.

This contribution will introduce the Flourish consortium and concept using the results of three years of active development, testing, and measuring in field campaigns. Two key parts of the project will be shown in more detail: First, mapping of the field by drones for detection of sugar beet nitrogen status variation and weed pressure in the field and second the perception of the UGV as related to weed classification and subsequent precision weed management.

The field mapping by means of an UAV will be shown for crop nitrogen status estimation and weed pressure with examples for subsequent crop management decision support. For nitrogen status, the results indicate that drones are up to the task to deliver crop nitrogen variability maps utilized for variable rate application that are of comparable quality to current on-tractor systems. The weed pressure mapping is viable as basis for the UGV showcase of precision weed management. For this, we show the automated image acquisition by the UGV and a subsequent plant classification with a four-step pipeline, differentiating crop from weed in real time. Advantages and disadvantages as well as future prospects of such approaches will be discussed.}
}

@article{potena2019agricolmap,
  title={AgriColMap: Aerial-Ground Collaborative 3D Mapping for Precision Farming},
  author={Potena, Ciro and Khanna, Raghav and Nieto, Juan and Siegwart, Roland and Nardi, Daniele and Pretto, Alberto},
  journal={IEEE Robotics and Automation Letters},
  year={2019},
  url={https://arxiv.org/abs/1810.00457},
  abstract={The combination of aerial survey capabilities of Unmanned Aerial Vehicles with targeted intervention abilities of agricultural Unmanned Ground Vehicles can significantly improve the effectiveness of robotic systems applied to precision agriculture. In this context, building and updating a common map of the field is an essential but challenging task. The maps built using robots of different types show differences in size, resolution and scale, the associated geolocation data may be inaccurate and biased, while the repetitiveness of both visual appearance and geometric structures found within agricultural contexts render classical map merging techniques ineffective. In this paper we propose AgriColMap, a novel map registration pipeline for that leverages a grid-based multi-modal environment representation which includes a vegetation index map and a Digital Surface Model. We cast the data association problem between maps built from UAVs and UGVs as a multi-modal, large displacement dense optical flow estimation. The dominant, coherent flows, selected using a voting scheme, are used as point-to-point correspondences to infer a preliminary non-rigid alignment between the maps. A final refinement is then performed, by exploiting only meaningful parts of the registered maps. We evaluate our system using real world data for 3 fields with different crop species. The results show that our method outperforms several state of the art map registration and matching techniques by a large margin, and has a higher tolerance to large initial misalignments. We release an implementation of the proposed approach along with the acquired datasets with this paper.},
  code={https://github.com/cirpote/AgriColMap},
  publisher={IEEE},
  categories={journal}
}