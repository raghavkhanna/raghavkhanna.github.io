<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Conference on Raghav Khanna</title>
    <link>https://raghavkhanna.github.io/categories/conference/</link>
    <description>Recent content in Conference on Raghav Khanna</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://raghavkhanna.github.io/categories/conference/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Low-Cost System for High-Rate, High-Accuracy Temporal Calibration for LIDARs and Cameras</title>
      <link>https://raghavkhanna.github.io/publications/sommer2017calibration_devices/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/sommer2017calibration_devices/</guid>
      <description>Deployment of camera and laser based motion estimation systems for controlling platforms operating at high speeds, such as cars or trains, is posing increasingly challenging precision requirements on the temporal calibration of these sensors. In this work, we demonstrate a simple, low-cost system for calibrating any combination of cameras and time of flight LIDARs with respect to the CPU clock (and therefore, also to each other). The newly proposed device is based on widely available off-the-shelf components, such as the Raspberry Pi 3, which is synchronized using the Precision Time Protocol (PTP) with respect to the CPU of the sensor carrying system.</description>
    </item>
    
    <item>
      <title>Build your own visual-inertial odometry aided cost-effective and open-source autonomous drone</title>
      <link>https://raghavkhanna.github.io/publications/sa2017build/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/sa2017build/</guid>
      <description>This paper describes an approach to building a cost-effective and research grade visual-inertial odometry aided vertical taking-off and landing (VTOL) platform. We utilize an off-the-shelf visual-inertial sensor, an onboard computer, and a quadrotor platform that are factory-calibrated and mass-produced, thereby sharing similar hardware and sensor specifications (e.g., mass, dimensions, intrinsic and extrinsic of camera-IMU systems, and signal-to-noise ratio). We then perform a system calibration and identification enabling the use of our visual-inertial odometry, multi-sensor fusion, and model predictive control frameworks with the off-the-shelf products.</description>
    </item>
    
    <item>
      <title>On field radiometric calibration for multispectral cameras</title>
      <link>https://raghavkhanna.github.io/publications/khanna2017field/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/khanna2017field/</guid>
      <description>Perception systems for outdoor robotics have to deal with varying environmental conditions. Variations in illumination in particular, are currently the biggest challenge for vision-based perception. In this paper we present an approach for radiometric characterization of multispectral cameras. To enable spatio-temporal mapping we also present a procedure for in-situ illumination estimation, resulting in radiometric calibration of the collected images. In contrast to current approaches, we present a purely data driven, parameter free approach, based on maximum likelihood estimation which can be performed entirely on the field, without requiring specialised laboratory equipment.</description>
    </item>
    
    <item>
      <title>UAV-based crop and weed classification for smart farming</title>
      <link>https://raghavkhanna.github.io/publications/lottes2017uav/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/lottes2017uav/</guid>
      <description>Unmanned aerial vehicles (UAVs) and other robots in smart farming applications offer the potential to monitor farm land on a per-plant basis, which in turn can reduce the amount of herbicides and pesticides that must be applied. A central information for the farmer as well as for autonomous agriculture robots is the knowledge about the type and distribu- tion of the weeds in the field. In this regard, UAVs offer excellent survey capabilities at low cost.</description>
    </item>
    
    <item>
      <title>Flourish-A robotic approach for automation in crop management</title>
      <link>https://raghavkhanna.github.io/publications/liebischflourish/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/liebischflourish/</guid>
      <description>The goal of the Flourish project is to bridge the gap between the current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. Thereby, combining the aerial survey capabilities of a small autono- mous multi-copter Unmanned Aerial Vehicle (UAV) with a multi-purpose agricultural Unmanned Ground Vehicle (UGV), the system will be able to survey a field from the air, perform targeted intervention on the ground, and provide detailed information for deci- sion support, all with minimal user intervention.</description>
    </item>
    
    <item>
      <title>Towards automatic UAV data interpretation for precision farming</title>
      <link>https://raghavkhanna.github.io/publications/pfeifer2016towards/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/pfeifer2016towards/</guid>
      <description>Background: The EU-project Flourish intends to establish an autonomously operating precision farming system based on the interaction between unmanned ground vehicles (UGVs) and unmanned aerial vehicles (UAVs). For effective mission planning and site-specific ground intervention by the UGV, the growth, mineral nutrition, weed, and health status of the crop field must be evaluated efficiently. In this regard, the survey capabilities of UAVs can substantially leverage the economic performance and ecological sustainability of precision farming systems.</description>
    </item>
    
    <item>
      <title>Beyond point clouds-3d mapping and field parameter measurements using uavs</title>
      <link>https://raghavkhanna.github.io/publications/khanna2015beyond/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/khanna2015beyond/</guid>
      <description>t developments in Unmanned Aerial Vehicles (UAVs) have made them ideal tools for remotely monitoring agricultural fields. Complementary advancements in computer vision have enabled automated post-processing of images to generate dense 3D reconstructions in the form of point clouds. In this paper we present a monitoring pipeline that uses a readily available, low cost UAV and camera for quickly surveying a winter wheat field, generate a 3D point cloud from the collected imagery and present methods for automated crop height estimation from the extracted point cloud and compare our estimates with those using standardized techniques.</description>
    </item>
    
    <item>
      <title>Studying Phenotypic Variability in Crops using a Hand-held Sensor Platform</title>
      <link>https://raghavkhanna.github.io/publications/khannastudying/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://raghavkhanna.github.io/publications/khannastudying/</guid>
      <description>t developments in visual-inertial and LiDAR sensors and simultaneous localization and mapping (SLAM) enable recording and digital reconstruction of the physical world. In this paper we utilize a hand-held multi-sensor platform for remotely recording and characterizing physical properties of crops on a field. The platform consists of a visual-inertial sensor, color camera and 2D LiDAR. We syncronize the data from this platform and fuse them in a standard SLAM framework to obtain a detailed model of the field environment in the form of a 3D point cloud.</description>
    </item>
    
  </channel>
</rss>